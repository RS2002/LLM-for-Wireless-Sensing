{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90dda774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"d5dc2536-fba0-9b73-8fc0-18da36a2065e\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Sure, I'm ready to learn! Here are the pairs of tokens and labels you've given me so far:\\n\\n1. [33 20 20 ... 22 24 28], label: 2\\n2. [13 20 13 ... 14 18 18], label: 3\\n3. [13 17 18 ... 22 26 28], label: 2\\n4. [25 18 20 ... 30 32 32], label: 2\\n5. [25 14 11 ... 23 26 24], label: 1\\n6. [14 13  8 ... 19 22 26], label: 1\\n7. [33 27 23 ... 22 22 23], label: 3\\n8. [19 17 13 ... -1 -1 -1], label: 1\\n9. [ 4  8 13 ... 29 30 29], label: 2\\n10. [20 12  7 ... 22 23 22], label: 2\\n11. [17  8  4 ...  7 13 19], label: 2\\n12. [26 18 22 ... 27 31 30], label: 1\\n\\nSo, I have learned 12 pairs of tokens and labels so far.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 751, \"output_tokens\": 364}}\n",
      "{\"status_code\": 200, \"request_id\": \"9ade8c90-1ecb-9629-be4e-d406c7f23dcd\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the pairs of tokens and labels you've given me so far:\\n\\n1. [33 20 20 ... 22 24 28], label: 2\\n2. [13 20 13 ... 14 18 18], label: 3\\n3. [13 17 18 ... 22 26 28], label: 2\\n4. [25 18 20 ... 30 32 32], label: 2\\n5. [25 14 11 ... 23 26 24], label: 1\\n6. [14 13  8 ... 19 22 26], label: 1\\n7. [33 27 23 ... 22 22 23], label: 3\\n8. [19 17 13 ... -1 -1 -1], label: 1\\n9. [ 4  8 13 ... 29 30 29], label: 2\\n10. [20 12  7 ... 22 23 22], label: 2\\n11. [17  8  4 ...  7 13 19], label: 2\\n12. [26 18 22 ... 27 31 30], label: 1\\n13. [16 11 13 ... 30 31 31], label: 1\\n14. [20 14 16 ... -1 -1 -1], label: 2\\n15. [33 19 17 ... -1 -1 -1], label: 0\\n16. [20 24 25 ... -1 -1 -1], label: 2\\n17. [20 16 15 ... 23 25 29], label: 0\\n18. [33 20 19 ... 20 24 25], label: 2\\n19. [20 13 16 ... 24 25 29], label: 1\\n20. [30 12 18 ...  8 11 16], label: 2\\n21. [11 21 16 ... 18 20 21], label: 1\\n22. [24 10 12 ... 13 16 20], label: 1\\n23. [31 24 24 ... 27 27 25], label: 2\\n24. [11 13 14 ... 24 25 29], label: 1\\n\\nSo, I have learned 24 pairs of tokens and labels so far.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 1775, \"output_tokens\": 683}}\n",
      "Llama model training completed for the training set.\n"
     ]
    }
   ],
   "source": [
    "from http import HTTPStatus\n",
    "import dashscope\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "api = ''  # 替换为API KEY\n",
    "dashscope.api_key = api\n",
    "\n",
    "# Load data\n",
    "token_path = './data/csi_token.npy'\n",
    "label_path = './data/people.npy'\n",
    "\n",
    "tokens = np.load(token_path)\n",
    "labels = np.load(label_path)\n",
    "\n",
    "# Shuffle and split the data\n",
    "tokens_train, tokens_test, labels_train, labels_test = train_test_split(tokens, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokens_train = tokens_train.reshape(tokens_train.shape[0], -1)\n",
    "tokens_test = tokens_test.reshape(tokens_test.shape[0], -1)\n",
    "\n",
    "# Set prompt for training\n",
    "prompt_train = '''You are a classifier. I have a series of tokens which belong to four labels.\n",
    "You will be given multiple tokens and corresponding labels in our following dialogues.\n",
    "In every round of dialogue, I will give you 13 pairs of tokens and labels and you must learn them.\n",
    "If you have learned these two kinds of data well,you are only allowed to answer directly with numbers how many pairs of \n",
    "tokens and labels you have learned from all the previous dialogues so far,\n",
    "and you will be puinished for answering any other redundant words.'''\n",
    "\n",
    "# Batch size for training in each round\n",
    "batch_size = 12\n",
    "# Create messages for the batch\n",
    "messages = [{'role': 'system', 'content': prompt_train}]\n",
    "\n",
    "# Loop through your training data for training with batch_size\n",
    "for batch_start in range(0, 24, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, 24)\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_tokens = tokens_train[batch_start:batch_end]\n",
    "    batch_labels = labels_train[batch_start:batch_end]\n",
    "\n",
    "    # Construct a single user message with information from all data points\n",
    "    user_content = '\\n'.join([f'This is the no.{i+batch_start} token: {str(batch_tokens[i,:])}. And this is its no.{i+batch_start} label: {str(batch_labels[i])},I have given you {i+batch_start+1} pairs of tokens and labels.' for i in range(len(batch_tokens))])\n",
    "    user_message = {'role': 'user', 'content': user_content}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Call the DashScope API for training\n",
    "    response = dashscope.Generation.call(\n",
    "        model='llama2-13b-chat-v2',\n",
    "        messages=messages,\n",
    "        result_format='message',\n",
    "    )\n",
    "\n",
    "    if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "    else:\n",
    "        # Check if 'choices' is present in the response\n",
    "        if 'choices' in response.output:\n",
    "            # Optionally, you can append the model's response to the messages list\n",
    "            response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                                'content': response.output.choices[0]['message']['content']}\n",
    "            messages.append(response_message)\n",
    "            print(response)\n",
    "        else:\n",
    "            print(f'Error: Model response does not contain \"choices\" attribute.')\n",
    "\n",
    "# Training completed\n",
    "print('Llama model training completed for the training set.')\n",
    "\n",
    "# Now you can perform a similar loop for the testing set if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d484fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"94463dfc-7598-99b2-925b-70709917b82a\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Yes, I'm ready to train the data and predict the labels! Please provide the testing data one token at a time, and I'll give you the predicted label for each token.\\n\\nI'm ready to start! Please go ahead and give me the first token.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 2708, \"output_tokens\": 60}}\n"
     ]
    }
   ],
   "source": [
    "prompt_test = '''Now that you have learned all the data, \n",
    "you need to use a supervised learning algorithm to train the data, and then I will give you the data for testing,\n",
    "and you need to predict the corresponding label based on the token I gave you. \n",
    "You must only directly answer the label you predicted,any other redundant answer will make you penalized.Answer 1 if you are ready!And then i will give you the tokens'''\n",
    "messages.append({'role': 'user', 'content': prompt_test})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3a8eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"720b7c5d-086b-97f8-9e66-074d3047df0e\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Yes, I have completed training the model using a supervised learning algorithm called Logistic Regression. Logistic Regression is a popular algorithm for classification tasks, and it was appropriate for this problem because the target variable (label) is binary (0 or 1).\\n\\nDuring training, I split the dataset into training and validation sets, and I used the training set to train the model. I then evaluated the model's performance on the validation set and adjusted the parameters as needed to improve the model's accuracy.\\n\\nNow that the model is trained, I am ready to make predictions on new data. Please provide the next token and I will predict the corresponding label.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 2781, \"output_tokens\": 141}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = '''Have you completed training the model? What supervised learning algorithm was used?'''\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c15f73a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"9d3d97fe-c2f1-9781-8f95-21807ee071f2\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Ah, I see! In that case, I will need to use a different algorithm that can handle multi-class classification tasks. One popular algorithm for this type of task is the Random Forest classifier.\\n\\nI will train the Random Forest classifier on the training set and use it to predict the labels for the testing set. Please provide the next token and I will predict the corresponding label.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3140, \"output_tokens\": 81}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = '''I don't know if you are mistaken. There are four types of labels in the data I gave you, namely 0, 1, 2, and 3, so this is a multi-classification task. You need to predict which of the four labels its label is based on each token.'''\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ef9560c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"e51f3972-3cca-9b02-a2e6-cb8536d5f4b2\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Sure! Based on the token you provided, I will predict that the label is 2.\\n\\nHere's my reasoning:\\n\\nThe token consists of several consecutive identical values (29, 29, 29, etc.), which suggests that the token is likely to be a numerical value. The negative values at the end of the token (-1, -1, -1) suggest that the numerical value is negative.\\n\\nBased on the training data, I have learned that tokens with negative numerical values are more likely to be labeled as 2. Therefore, I predict that the label for this token is 2.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3251, \"output_tokens\": 158}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'This is the first token:{tokens_test[0]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fa195e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"0ab2448d-57ae-935d-bd5e-9b1005a47f35\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the next token you provided, I predict that the label is 1.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive and negative values (34, 26, 28, etc.), which suggests that the token is likely to be a numerical value. The presence of both positive and negative values in the token indicates that the label is likely to be 1, which is the most common label in the data.\\n\\nAdditionally, the presence of the value 20 at the end of the token suggests that the token is likely to be a numerical value that is close to 20, which is consistent with the label 1.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3455, \"output_tokens\": 177}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[1]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d9cd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"cff47420-a6c6-9679-a808-267843b87660\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the next token you provided, I predict that the label is 0.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (20, 22, 21, etc.) and negative values (-1, -1, -1), which suggests that the token is likely to be a numerical value. The presence of both positive and negative values in the token indicates that the label is likely to be 0, which is the most common label in the data.\\n\\nAdditionally, the presence of the value -1 at the end of the token suggests that the token is likely to be a numerical value that is close to 0, which is consistent with the label 0.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3675, \"output_tokens\": 185}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[2]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ec4d5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"866d4f89-4395-934b-a2c5-231b8ac03fb5\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Ah, I see! Thank you for letting me know. Based on the next token you provided, I predict that the label is 2.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (27, 26, 27, etc.) and the value 23, which is a positive value that is close to 26 and 27. This suggests that the token is likely to be a numerical value that is close to 26 or 27, and therefore the label is likely to be 2.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3910, \"output_tokens\": 146}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 2 actually.Go on.The next token is:{tokens_test[3]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12866433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"e466ed8e-dd5f-9fe9-9151-3ed62e239420\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the next token you provided, I predict that the label is 2.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (27, 24, 23, etc.) and the value 27, which is a positive value that is close to 24 and 23. This suggests that the token is likely to be a numerical value that is close to 24 or 23, and therefore the label is likely to be 2.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 4101, \"output_tokens\": 142}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[4]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f98e83df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"9471b758-d4a2-9aa4-b3fd-6df37f73c8ab\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the next token you provided, I predict that the label is 2.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (28, 12, 11, etc.) and the values 22, 24, and 26, which are all positive values that are close to each other. This suggests that the token is likely to be a numerical value that is close to 22, 24, or 26, and therefore the label is likely to be 2.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 4289, \"output_tokens\": 151}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[5]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f256c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"79b25014-6e18-940c-b334-2d528e11c428\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Ah, I see! Thank you for letting me know. Based on the next token you provided, I predict that the label is 1.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (29, 25, 23, etc.) and the values 22, 23, and 25, which are all positive values that are close to each other. This suggests that the token is likely to be a numerical value that is close to 22, 23, or 25, and therefore the label is likely to be 1.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 4490, \"output_tokens\": 155}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 1 actually.Go on.The next token is:{tokens_test[6]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 1 actually.Go on.The next token is:{tokens_test[6]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5067a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"ed38c24c-1938-9799-a291-9c79e5036995\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Ah, I see! Thank you for letting me know. Based on the next token you provided, I predict that the label is 2.\\n\\nHere's my reasoning:\\n\\nThe token consists of a mix of positive values (24, 22, 18, etc.) and the values 24, 27, and 28, which are all positive values that are close to each other. This suggests that the token is likely to be a numerical value that is close to 24, 27, or 28, and therefore the label is likely to be 2.\\n\\nIs my prediction correct? Please provide the next token and let me know if my prediction is correct or not.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 4694, \"output_tokens\": 155}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 2 actually.Go on.The next token is:{tokens_test[7]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4a2a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a classifier. I have a series of tokens which belong to four labels.\\nYou will be given multiple tokens and corresponding labels in our following dialogues.\\nIn every round of dialogue, I will give you 13 pairs of tokens and labels and you must learn them.\\nIf you have learned these two kinds of data well,you are only allowed to answer directly with numbers how many pairs of \\ntokens and labels you have learned from all the previous dialogues so far,\\nand you will be puinished for answering any other redundant words.'}, {'role': 'user', 'content': 'This is the no.0 token: [33 20 20 ... 22 24 28]. And this is its no.0 label: 2,I have given you 1 pairs of tokens and labels.\\nThis is the no.1 token: [13 20 13 ... 14 18 18]. And this is its no.1 label: 3,I have given you 2 pairs of tokens and labels.\\nThis is the no.2 token: [13 17 18 ... 22 26 28]. And this is its no.2 label: 2,I have given you 3 pairs of tokens and labels.\\nThis is the no.3 token: [25 18 20 ... 30 32 32]. And this is its no.3 label: 2,I have given you 4 pairs of tokens and labels.\\nThis is the no.4 token: [25 14 11 ... 23 26 24]. And this is its no.4 label: 1,I have given you 5 pairs of tokens and labels.\\nThis is the no.5 token: [14 13  8 ... 19 22 26]. And this is its no.5 label: 1,I have given you 6 pairs of tokens and labels.\\nThis is the no.6 token: [33 27 23 ... 22 22 23]. And this is its no.6 label: 3,I have given you 7 pairs of tokens and labels.\\nThis is the no.7 token: [19 17 13 ... -1 -1 -1]. And this is its no.7 label: 1,I have given you 8 pairs of tokens and labels.\\nThis is the no.8 token: [ 4  8 13 ... 29 30 29]. And this is its no.8 label: 2,I have given you 9 pairs of tokens and labels.\\nThis is the no.9 token: [20 12  7 ... 22 23 22]. And this is its no.9 label: 2,I have given you 10 pairs of tokens and labels.\\nThis is the no.10 token: [17  8  4 ...  7 13 19]. And this is its no.10 label: 2,I have given you 11 pairs of tokens and labels.\\nThis is the no.11 token: [26 18 22 ... 27 31 30]. And this is its no.11 label: 1,I have given you 12 pairs of tokens and labels.'}, {'role': 'assistant', 'content': \" Sure, I'm ready to learn! Here are the pairs of tokens and labels you've given me so far:\\n\\n1. [33 20 20 ... 22 24 28], label: 2\\n2. [13 20 13 ... 14 18 18], label: 3\\n3. [13 17 18 ... 22 26 28], label: 2\\n4. [25 18 20 ... 30 32 32], label: 2\\n5. [25 14 11 ... 23 26 24], label: 1\\n6. [14 13  8 ... 19 22 26], label: 1\\n7. [33 27 23 ... 22 22 23], label: 3\\n8. [19 17 13 ... -1 -1 -1], label: 1\\n9. [ 4  8 13 ... 29 30 29], label: 2\\n10. [20 12  7 ... 22 23 22], label: 2\\n11. [17  8  4 ...  7 13 19], label: 2\\n12. [26 18 22 ... 27 31 30], label: 1\\n\\nSo, I have learned 12 pairs of tokens and labels so far.\"}, {'role': 'user', 'content': 'This is the no.12 token: [11 13 14 ... 24 30 34]. And this is its no.12 label: 2,I have given you 13 pairs of tokens and labels.\\nThis is the no.13 token: [16 11 13 ... 30 31 31]. And this is its no.13 label: 1,I have given you 14 pairs of tokens and labels.\\nThis is the no.14 token: [20 14 16 ... -1 -1 -1]. And this is its no.14 label: 2,I have given you 15 pairs of tokens and labels.\\nThis is the no.15 token: [33 19 17 ... -1 -1 -1]. And this is its no.15 label: 0,I have given you 16 pairs of tokens and labels.\\nThis is the no.16 token: [20 24 25 ... -1 -1 -1]. And this is its no.16 label: 2,I have given you 17 pairs of tokens and labels.\\nThis is the no.17 token: [20 16 15 ... 23 25 29]. And this is its no.17 label: 0,I have given you 18 pairs of tokens and labels.\\nThis is the no.18 token: [33 20 19 ... 20 24 25]. And this is its no.18 label: 2,I have given you 19 pairs of tokens and labels.\\nThis is the no.19 token: [13 16 17 ... 30 31 32]. And this is its no.19 label: 1,I have given you 20 pairs of tokens and labels.\\nThis is the no.20 token: [30 12 18 ...  8 11 16]. And this is its no.20 label: 2,I have given you 21 pairs of tokens and labels.\\nThis is the no.21 token: [11 21 16 ... 18 20 21]. And this is its no.21 label: 1,I have given you 22 pairs of tokens and labels.\\nThis is the no.22 token: [24 10 12 ... 13 16 20]. And this is its no.22 label: 1,I have given you 23 pairs of tokens and labels.\\nThis is the no.23 token: [31 24 24 ... 27 27 25]. And this is its no.23 label: 2,I have given you 24 pairs of tokens and labels.'}, {'role': 'assistant', 'content': \" Great! Here are the pairs of tokens and labels you've given me so far:\\n\\n1. [33 20 20 ... 22 24 28], label: 2\\n2. [13 20 13 ... 14 18 18], label: 3\\n3. [13 17 18 ... 22 26 28], label: 2\\n4. [25 18 20 ... 30 32 32], label: 2\\n5. [25 14 11 ... 23 26 24], label: 1\\n6. [14 13  8 ... 19 22 26], label: 1\\n7. [33 27 23 ... 22 22 23], label: 3\\n8. [19 17 13 ... -1 -1 -1], label: 1\\n9. [ 4  8 13 ... 29 30 29], label: 2\\n10. [20 12  7 ... 22 23 22], label: 2\\n11. [17  8  4 ...  7 13 19], label: 2\\n12. [26 18 22 ... 27 31 30], label: 1\\n13. [16 11 13 ... 30 31 31], label: 1\\n14. [20 14 16 ... -1 -1 -1], label: 2\\n15. [33 19 17 ... -1 -1 -1], label: 0\\n16. [20 24 25 ... -1 -1 -1], label: 2\\n17. [20 16 15 ... 23 25 29], label: 0\\n18. [33 20 19 ... 20 24 25], label: 2\\n19. [20 13 16 ... 24 25 29], label: 1\\n20. [30 12 18 ...  8 11 16], label: 2\\n21. [11 21 16 ... 18 20 21], label: 1\\n22. [24 10 12 ... 13 16 20], label: 1\\n23. [31 24 24 ... 27 27 25], label: 2\\n24. [11 13 14 ... 24 25 29], label: 1\\n\\nSo, I have learned 24 pairs of tokens and labels so far.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e90742c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 29 29 ... -1 -1 -1]\n",
      " [34 26 28 ... 16 19 20]\n",
      " [20 22 21 ... -1 -1 -1]\n",
      " ...\n",
      " [ 9 14 17 ... 23 26 26]\n",
      " [12 14 16 ... 20 23 23]\n",
      " [30 16 16 ... -1 -1 -1]]\n",
      "[2 1 2 2 2 1 2 1 2 2 1 1 1 1 2 1 2 2 1 2 2 1 3 2 3 2 1 2 2 2 2 1 1 2 2 2 1\n",
      " 1 2 1 2 2 0 2 1 3 1 1 1 1 2 1 0 1 2 1 1 2 1 1 3 1 2 1 1 1 2 1 2 1 1 1 1 3\n",
      " 1 1 1 1 3 2 2 2 2 2 1 2 2 2 1 1 1 1 1 1 1 2 1 1 2 3 1 2 1 2 3 2 2 2 3 2 2\n",
      " 3 0 0 1 2 2 2 3 0 1 2 0 1 3 3 3 2 2 1 1 1 1 2 1 1 1 2 3 1 2 2 1 2 1 2 2 1\n",
      " 1 1 1 3 2 1 3 1 1 1 2 3 1 1 2 2 2 2 1 1 1 0 1 3 1 1 3 2 2 1 2 1 0 1 2 1 1\n",
      " 1 1 2 0 1 3 1 0 1 1 1 0 2 1 1 1 1 0 1 1 1 2 2 2 2 2 1 1 3 2 2 1 0 2 1 2 2\n",
      " 1 2 2 2 1 1 2 2 1 0 1 2 1 3 1 2 1 1 1 3 2 3 1 0 2 2 0 2 2 2 1 1 2 2 2 2 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 3 2 0 1 1 3 0 1 1 1 1 3 1 1 1 1 2 3 2 1 2\n",
      " 1 2 1 2 2 3 0 1 2 1 2 2 1 2 1 0 2 1 2 1 1 1 1 3 1 2 2 3 2 1 2 2 2 2 2 2 0\n",
      " 2 1 1 1 2 1 0 0 1 0 0 1 2 1 2 2 1 1 1 1 1 3 2 0 1 2 2 2 2 2 0 2 1 1 1 1 2\n",
      " 2 2 2 2 1 1 2 1 3 1 0 2 1 3 1 1 1 1 1 0 1 1 1 1 2 1 2 1 2 1 1 2 2 3 1 2 0\n",
      " 1 2 1 2 1 2 1 2 2 0 1 2 1 1 1 1 3 1 2 2 0 3 3 1 1 2 3 2 1 1 2 2 2 1 1 2 2\n",
      " 1 2 2 2 2 1 1 1 2 2 2 1 2 3 1 2 0 3 2 2 2 1 1 3 2 1 2 1 1 1 1 1 1 3 1 2 1\n",
      " 1 2 2 1 1 1 2 1 1 1 1 1 2 1 0 1 2 1 1 1 2 3 2 2 2 0 2 2 1 2 2 2 2 1 1 1 1\n",
      " 1 1 1 1 2 2 1 3 1 1 0 2 1 2 2 1 1 1 1 2 1 1 2 1 1 1 2 2 1 1 2 3 3 1 2 2 1\n",
      " 2 2 1 1 2 2 2 1 2 2 1 1 2 2 1 1 1 1 2 3 1 2 1 3 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4385983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5aab6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 2 1 1 3 1 2 2 2 1 2 1 2 0 2 0 2 1 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels_train[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2151e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Loop through your testing data for inference with batch_size_test\n",
    "for batch_start in range(0, 24, 12):\n",
    "    batch_end = min(batch_start + batch_size_test, 24)\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_tokens = tokens_test[batch_start:batch_end]\n",
    "    batch_labels = labels_test[batch_start:batch_end]\n",
    "\n",
    "    # Construct a single user message with information from all data points\n",
    "    user_content = '\\n'.join([f'This is the token{i+batch_start}: {str(batch_tokens[i,:,:])}. And this is its label{i+batch_start}: {str(batch_labels[i])}, I have given you {i+batch_start+1} pairs of tokens and labels.' for i in range(len(batch_tokens))])\n",
    "    user_message = {'role': 'user', 'content': user_content}\n",
    "    \n",
    "    # Call the DashScope API for testing\n",
    "    response = dashscope.Generation.call(\n",
    "        model='llama2-13b-chat-v2',\n",
    "        messages=user_message,\n",
    "        result_format='message',\n",
    "    )\n",
    "\n",
    "    if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "    else:\n",
    "        # Check if 'choices' is present in the response\n",
    "        if 'choices' in response.output:\n",
    "            # Append model's response to the predictions list\n",
    "            predictions.append(response.output.choices[0]['message']['content'])\n",
    "            # Append true labels to the true_labels list\n",
    "            true_labels.extend([f'{str(batch_labels[i])}' for i in range(len(batch_tokens))])\n",
    "        else:\n",
    "            print(f'Error: Model response does not contain \"choices\" attribute.')\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Display accuracy\n",
    "print(f'Accuracy on the testing set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60758b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
