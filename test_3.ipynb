{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f871949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.09 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from http import HTTPStatus\n",
    "import dashscope\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "api = ''  # 替换为API KEY\n",
    "dashscope.api_key = api\n",
    "\n",
    "# Load data\n",
    "token_path = './data/csi_token.npy'\n",
    "label_path = './data/people.npy'\n",
    "\n",
    "tokens = np.load(token_path)\n",
    "labels = np.load(label_path)\n",
    "\n",
    "# Shuffle and split the data\n",
    "tokens_train, tokens_test, labels_train, labels_test = train_test_split(tokens, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokens_train = tokens_train.reshape(tokens_train.shape[0], -1)\n",
    "tokens_test = tokens_test.reshape(tokens_test.shape[0], -1)\n",
    "\n",
    "# Set prompt for training\n",
    "prompt_train = '''You are a classifier. I have a series of tokens which belong to four labels.\n",
    "You will be given multiple tokens and corresponding labels in our following dialogues.\n",
    "In every round of dialogue, I will give you 13 pairs of tokens and labels and you must learn them.\n",
    "If you have learned these two kinds of data well,you are only allowed to answer directly with numbers how many pairs of \n",
    "tokens and labels you have learned from all the previous dialogues so far,\n",
    "and you will be puinished for answering any other redundant words.'''\n",
    "\n",
    "# Batch size for training in each round\n",
    "batch_size = 12\n",
    "# Create messages for the batch\n",
    "messages = [{'role': 'system', 'content': prompt_train}]\n",
    "\n",
    "# Loop through your training data for training with batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0836eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"8fc3afc9-a3e4-9382-a1a2-97f6b500e228\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Sure, I'm ready to learn! Here are the 13 pairs of tokens and labels you've given me so far:\\n\\n1. [26 14 15 ... 21 25 27] - label: 1\\n2. [28  6  9 ... 21 23 24] - label: 2\\n3. [ 3 26 24 ... 21 24 25] - label: 2\\n4. [15 16 13 ... 26 28 27] - label: 1\\n5. [ 4 15 16 ... 21 24 25] - label: 2\\n6. [30 12 13 ... 18 23 26] - label: 1\\n7. [13 22 20 ... 20 24 26] - label: 1\\n8. [23 13  9 ... 21 22 24] - label: 1\\n9. [33 16 18 ... 26 29 30] - label: 0\\n10. [13 16 13 ... 26 28 28] - label: 2\\n11. [16 24 27 ... 22 24 25] - label: 2\\n12. [20 11  6 ... -1 -1 -1] - label: 2\\n\\nSo, after these 13 pairs of tokens and labels, I have learned 12 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 638, \"output_tokens\": 390}}\n",
      "{\"status_code\": 200, \"request_id\": \"5c356d8e-9157-944d-99f5-d2f0304e1758\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 13 pairs of tokens and labels you've given me so far:\\n\\n1. [26 14 15 ... 21 25 27] - label: 1\\n2. [28  6  9 ... 21 23 24] - label: 2\\n3. [ 3 26 24 ... 21 24 25] - label: 2\\n4. [15 16 13 ... 26 28 27] - label: 1\\n5. [ 4 15 16 ... 21 24 25] - label: 2\\n6. [30 12 13 ... 18 23 26] - label: 1\\n7. [13 22 20 ... 20 24 26] - label: 1\\n8. [23 13  9 ... 21 22 24] - label: 1\\n9. [33 16 18 ... 26 29 30] - label: 0\\n10. [13 16 13 ... 26 28 28] - label: 2\\n11. [16 24 27 ... 22 24 25] - label: 2\\n12. [20 11  6 ... -1 -1 -1] - label: 2\\n13. [31 26 26 ... 28 31 33] - label: 2\\n14. [27 21 21 ... 19 23 26] - label: 1\\n15. [30 18 15 ... 27 28 29] - label: 2\\n16. [20 16 13 ... 26 26 26] - label: 1\\n17. [13 14 13 ... 20 22 27] - label: 1\\n18. [10 13 13 ... 18 22 24] - label: 1\\n19. [19 16 15 ... 24 27 27] - label: 2\\n20. [23 32 32 ... 23 29 31] - label: 2\\n21. [30 18 16 ... 26 28 30] - label: 1\\n22. [11  9 12 ... 18 22 25] - label: 1\\n23. [11 25 22 ... 13 16 16] - label: 3\\n24. [26 18 18 ... 24 27 27] - label: 2\\n25. [23 32 32 ... 23 29 31] - label: 2\\n\\nSo, after these 13 pairs of tokens and labels, I have learned 14 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 1550, \"output_tokens\": 759}}\n",
      "{\"status_code\": 200, \"request_id\": \"b6d3c6f4-2f0d-95e4-bef7-0b4167bf074a\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 15 pairs of tokens and labels you've given me so far:\\n\\n1. [26 24 26 ... 18 22 24] - label: 2\\n2. [10  7  6 ... 22 21 21] - label: 1\\n3. [21 16 16 ... -1 -1 -1] - label: 1\\n4. [37 22 20 ... 23 24 25] - label: 2\\n5. [12 22 12 ... 19 21 22] - label: 1\\n6. [15 29 25 ... 28 32 34] - label: 1\\n7. [ 9 20 19 ... 20 21 21] - label: 2\\n8. [27 23 20 ... 28 30 30] - label: 2\\n9. [19 17 18 ... -1 -1 -1] - label: 1\\n10. [12 20 18 ... 21 24 24] - label: 1\\n11. [23 17 18 ... 33 35 37] - label: 3\\n12. [20 23 25 ... 12 16 18] - label: 2\\n\\nSo, after these 15 pairs of tokens and labels, I have learned 16 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 2827, \"output_tokens\": 382}}\n",
      "{\"status_code\": 200, \"request_id\": \"f2b1f3a4-093d-9fd9-8888-2cee64783830\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 16 pairs of tokens and labels you've given me so far:\\n\\n1. [26 24 26 ... 18 22 24] - label: 2\\n2. [10  7  6 ... 22 21 21] - label: 1\\n3. [21 16 16 ... -1 -1 -1] - label: 1\\n4. [37 22 20 ... 23 24 25] - label: 2\\n5. [12 22 12 ... 19 21 22] - label: 1\\n6. [15 29 25 ... 28 32 34] - label: 1\\n7. [ 9 20 19 ... 20 21 21] - label: 2\\n8. [27 23 20 ... 28 30 30] - label: 2\\n9. [19 17 18 ... -1 -1 -1] - label: 1\\n10. [20 20 20 ... 27 27 27] - label: 1\\n11. [27  7  6 ... 18 22 24] - label: 1\\n12. [ 7 19 19 ... 24 28 30] - label: 0\\n13. [29 12 18 ... 21 26 28] - label: 2\\n14. [19 14 16 ... 26 29 27] - label: 1\\n15. [20 18 17 ... 28 30 30] - label: 1\\n16. [22 14 13 ... 17 20 20] - label: 1\\n\\nSo, after these 16 pairs of tokens and labels, I have learned 17 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 3724, \"output_tokens\": 496}}\n",
      "{\"status_code\": 200, \"request_id\": \"f6258fe2-8102-920b-bb85-1d50243cee0e\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 17 pairs of tokens and labels you've given me so far:\\n\\n1. [26 24 26 ... 18 22 24] - label: 2\\n2. [10  7  6 ... 22 21 21] - label: 1\\n3. [21 16 16 ... -1 -1 -1] - label: 1\\n4. [37 22 20 ... 23 24 25] - label: 2\\n5. [12 22 12 ... 19 21 22] - label: 1\\n6. [15 29 25 ... 28 32 34] - label: 1\\n7. [ 9 20 19 ... 20 21 21] - label: 2\\n8. [27 23 20 ... 28 30 30] - label: 2\\n9. [19 17 18 ... -1 -1 -1] - label: 1\\n10. [20 20 20 ... 27 27 27] - label: 1\\n11. [27  7  6 ... 18 22 24] - label: 1\\n12. [ 7 19 19 ... 24 28 30] - label: 0\\n13. [29 12 18 ... 21 26 28] - label: 2\\n14. [19 14 16 ... 26 29 27] - label: 1\\n15. [20 18 17 ... 28 30 30] - label: 1\\n16. [22 14 13 ... 17 20 20] - label: 1\\n17. [12 20 20 ... 23 26 30] - label: 2\\n\\nSo, after these 17 pairs of tokens and labels, I have learned 19 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 4737, \"output_tokens\": 525}}\n",
      "{\"status_code\": 200, \"request_id\": \"2866281d-cbe0-93fb-8c56-a4a845ee59bc\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 19 pairs of tokens and labels you've given me so far:\\n\\n1. [26 24 26 ... 18 22 24] - label: 2\\n2. [10  7  6 ... 22 21 21] - label: 1\\n3. [21 16 16 ... -1 -1 -1] - label: 1\\n4. [37 22 20 ... 23 24 25] - label: 2\\n5. [12 22 12 ... 19 21 22] - label: 1\\n6. [15 29 25 ... 28 32 34] - label: 1\\n7. [ 9 20 19 ... 20 21 21] - label: 2\\n8. [27 23 20 ... 28 30 30] - label: 2\\n9. [19 17 18 ... -1 -1 -1] - label: 1\\n10. [20 20 20 ... 27 27 27] - label: 1\\n11. [27  7  6 ... 18 22 24] - label: 1\\n12. [ 7 19 19 ... 24 28 30] - label: 0\\n13. [29 12 18 ... 21 26 28] - label: 2\\n14. [19 14 16 ... 26 29 27] - label: 1\\n15. [20 18 17 ... 28 30 30] - label: 1\\n16. [22 14 13 ... 17 20 20] - label: 1\\n17. [12 20 20 ... 23 26 30] - label: 2\\n18. [21 16 17 ... -1 -1 -1] - label: 2\\n19. [ 8 18 14 ... 24 24 24] - label: 2\\n\\nSo, after these 19 pairs of tokens and labels, I have learned 21 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 5777, \"output_tokens\": 580}}\n",
      "{\"status_code\": 200, \"request_id\": \"ac697f5f-1d42-9783-b024-b81421a4c554\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 10 pairs of tokens and labels you've given me so far:\\n\\n1. [24 26 27 ... 12 14 14] - label: 1\\n2. [15 12 11 ... 23 24 24] - label: 2\\n3. [10  9  8 ... 26 29 29] - label: 1\\n4. [16 23 24 ... 16 18 21] - label: 2\\n5. [34 24 26 ... 21 24 26] - label: 2\\n6. [31 16 17 ... -1 -1 -1] - label: 1\\n7. [33 20 20 ... 21 23 24] - label: 1\\n8. [ 7 20 24 ... 19 23 24] - label: 2\\n9. [16 17 16 ... -1 -1 -1] - label: 1\\n10. [24 12 15 ... 23 21 22] - label: 2\\n\\nSo, after these 10 pairs of tokens and labels, I have learned 31 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 6872, \"output_tokens\": 324}}\n",
      "{\"status_code\": 200, \"request_id\": \"a0389e5a-716b-9132-8c81-9ebd571fe5f8\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 12 pairs of tokens and labels you've given me so far:\\n\\n1. [27 12 12 ... 16 20 23] - label: 1\\n2. [13 18 18 ... 27 29 28] - label: 1\\n3. [ 8 22 20 ... 20 22 22] - label: 2\\n4. [26 16 14 ... 25 27 27] - label: 1\\n5. [26 24 24 ... 17 20 21] - label: 2\\n6. [ 4 18 17 ... 26 28 27] - label: 1\\n7. [34 22 20 ... 24 27 28] - label: 2\\n8. [31 28 24 ... 22 24 25] - label: 2\\n9. [25  9  8 ... 12 19 23] - label: 2\\n10. [23 21 21 ... 18 20 22] - label: 2\\n11. [24 20 20 ... 20 22 24] - label: 2\\n12. [24 24 26 ... 24 26 27] - label: 2\\n\\nSo, after these 12 pairs of tokens and labels, I have learned 44 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 7720, \"output_tokens\": 388}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"5230848f-7c02-9a6b-86b6-4a70d1af255c\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 12 pairs of tokens and labels you've given me so far:\\n\\n1. [28 24 22 ... -1 -1 -1] - label: 2\\n2. [23 15 12 ... 21 21 23] - label: 3\\n3. [21 15 14 ... 27 29 30] - label: 1\\n4. [21 11 12 ... 10 16 22] - label: 2\\n5. [21 18 19 ... 27 30 30] - label: 1\\n6. [11 11 10 ... 30 32 30] - label: 1\\n7. [25 12 16 ... 22 22 24] - label: 1\\n8. [30 25 22 ... 24 29 29] - label: 2\\n9. [19 21 21 ... 25 28 31] - label: 2\\n10. [27 15 16 ... 21 23 24] - label: 1\\n11. [15  4  8 ... 10 12 14] - label: 2\\n12. [23  7 10 ... 22 23 25] - label: 1\\n\\nSo, after these 12 pairs of tokens and labels, I have learned 48 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 8628, \"output_tokens\": 384}}\n",
      "{\"status_code\": 200, \"request_id\": \"ecb070a1-5fd1-92ee-b3fd-b3764a74c75a\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Great! Here are the 12 pairs of tokens and labels you've given me so far:\\n\\n1. [28 24 22 ... -1 -1 -1] - label: 2\\n2. [23 15 12 ... 21 21 23] - label: 3\\n3. [21 15 14 ... 27 29 30] - label: 1\\n4. [21 11 12 ... 10 16 22] - label: 2\\n5. [21 18 19 ... 27 30 30] - label: 1\\n6. [11 11 10 ... 30 32 30] - label: 1\\n7. [25 12 16 ... 22 22 24] - label: 1\\n8. [30 25 22 ... 24 29 29] - label: 2\\n9. [19 21 21 ... 25 28 31] - label: 2\\n10. [27 15 16 ... 21 23 24] - label: 1\\n11. [15  4  8 ... 10 12 14] - label: 2\\n12. [23  7 10 ... 22 23 25] - label: 1\\n\\nSo, after these 12 pairs of tokens and labels, I have learned 48 pairs of tokens and labels.\\n\\nNow, here are the 12 new pairs of tokens and labels you've given me:\\n\\n1. [30 22 20 ... 22 22 23] - label: 2\\n2. [18 18 18 ... 20 22 26] - label: 1\\n3. [26  7 13 ... -1 -1 -1] - label: 1\\n4. [17 18 17 ... 23 24 26] - label: 1\\n5. [13 14 15 ... 24 29 29] - label: 2\\n6. [30 24 24 ... 15 18 24] - label: 2\\n7. [35 16 16 ... 14 18 22] - label: 2\\n8. [18 16 15 ... 20 22 26] - label: 1\\n9. [28 17 15 ... 25 27 27] - label: 1\\n10. [34  9  9 ... 23 26 25] - label: 2\\n11. [12 18 21 ... 21 22 23] - label: 1\\n12. [27 16 11 ... -1 -1 -1] - label: 2\\n\\nSo, after these 12 new pairs of tokens and labels, I have learned a total of 60 pairs of tokens and labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 9529, \"output_tokens\": 768}}\n",
      "Llama model training completed for the training set.\n",
      "CPU times: total: 2.34 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for batch_start in range(120, 240, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, 240)\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_tokens = tokens_train[batch_start:batch_end]\n",
    "    batch_labels = labels_train[batch_start:batch_end]\n",
    "\n",
    "    # Construct a single user message with information from all data points\n",
    "    user_content = '\\n'.join([f'This is the next token: {str(batch_tokens[i,:])}， and this is its corresponding label: {str(batch_labels[i])}' for i in range(len(batch_tokens))])\n",
    "    user_message = {'role': 'user', 'content': user_content}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Call the DashScope API for training\n",
    "    response = dashscope.Generation.call(\n",
    "        model='llama2-13b-chat-v2',\n",
    "        messages=messages,\n",
    "        result_format='message',\n",
    "    )\n",
    "\n",
    "    if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "    else:\n",
    "        # Check if 'choices' is present in the response\n",
    "        if 'choices' in response.output:\n",
    "            # Optionally, you can append the model's response to the messages list\n",
    "            response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                                'content': response.output.choices[0]['message']['content']}\n",
    "            messages.append(response_message)\n",
    "            print(response)\n",
    "        else:\n",
    "            print(f'Error: Model response does not contain \"choices\" attribute.')\n",
    "\n",
    "# Training completed\n",
    "print('Llama model training completed for the training set.')\n",
    "\n",
    "# Now you can perform a similar loop for the testing set if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e73831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"17268e1c-e345-9d35-8982-92c7e3c8b00b\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Yes, I'm ready to train the data and predict the labels! Please provide the tokens for testing.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10388, \"output_tokens\": 24}}\n"
     ]
    }
   ],
   "source": [
    "prompt_test = '''Now that you have learned all the data, \n",
    "you need to use a supervised learning algorithm to train the data, and then I will give you the data for testing,\n",
    "and you need to predict the corresponding label based on the token I gave you. \n",
    "You must only directly answer the label you predicted,any other redundant answer will make you penalized.Answer 1 if you are ready!And then i will give you the tokens'''\n",
    "messages.append({'role': 'user', 'content': prompt_test})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afbf1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"214c5dfd-2b60-946a-9657-aa9e713776a0\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \" Yes, I have completed training the model using a supervised learning algorithm called Logistic Regression. Logistic Regression is a popular algorithm for classification tasks, where the goal is to predict a binary label based on input features. In this case, the input features are the token sequences, and the binary label is the corresponding label (1 or 2).\\n\\nDuring training, I split the dataset into training and validation sets, with a ratio of 80% for training and 20% for validation. I then used the training set to train the model, and evaluated the model's performance on the validation set after each epoch. I trained the model for a total of 10 epochs, and the best model was selected based on the validation loss.\\n\\nNow, I am ready to make predictions on the test set. Please provide the tokens for testing.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10425, \"output_tokens\": 183}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = '''Have you completed training the model? What supervised learning algorithm was used?'''\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec643f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"b7ca4749-934d-971b-a004-bae495603b92\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oh, I see! In that case, I apologize for the confusion. I will need to use a different algorithm that can handle multi-class classification tasks, such as a Support Vector Machine (SVM) or a Random Forest classifier.\\n\\nPlease provide me with the testing data, and I will use these algorithms to predict the labels for each token.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 1687, \"output_tokens\": 75}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = '''I don't know if you are mistaken. There are four types of labels in the data, namely 0, 1, 2, and 3, so this is a multi-classification task. You need to predict which of the four labels its label is based on each token.'''\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc41bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"7dd322c2-90af-97f9-a31b-9b98861b884d\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Based on the input token sequence `[29 29 29 ... -1 -1 -1]`, I predict that the label is `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10637, \"output_tokens\": 34}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'This is the first token:{tokens_test[0]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdca3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"2824d93f-95dc-9840-839b-2e86f2387b06\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the input token sequence `[28 12 11 ... 22 24 26]`, I predict that the label is `1`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10717, \"output_tokens\": 45}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[5]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages, \n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ad44cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"32669a5c-78bc-9342-9896-4f5734aa4f77\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Awesome, thank you for the reward! Based on the input token sequence `[34 26 28 ... 16 19 20]`, I predict that the label is `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10808, \"output_tokens\": 46}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[1]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages, \n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06898df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"ae42eaf5-d020-9d59-8a0e-0a853d544dfa\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oops, looks like I made a mistake! Based on the input token sequence `[20 22 21 ... -1 -1 -1]`, I predict that the label is `1` because the sequence contains a mix of positive and negative numbers, which is more similar to the characteristics of label `1`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 10906, \"output_tokens\": 68}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 1 actually.Go on.The next token is:{tokens_test[2]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1146b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"469728e4-7776-92c8-8ac4-057bed948039\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Alright, here's my prediction for the next token sequence:\\n\\nBased on the input token sequence `[27 26 27 ... 23 26 27]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the number `27` twice in the sequence suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11028, \"output_tokens\": 89}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 1 actually.Go on.The next token is:{tokens_test[3]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "741edd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"b56121d4-be10-9d45-8272-2f21cd555a05\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Here's my prediction for the next token sequence:\\n\\nBased on the input token sequence `[27 24 23 ... 27 25 23]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the number `27` three times in the sequence suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11162, \"output_tokens\": 95}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[4]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38253c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"4f3dca34-c21d-9049-a14d-1b1a095ea677\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Awesome, thank you for the reward! Here's my prediction for the next token sequence:\\n\\nBased on the input token sequence `[29 25 23 ... 22 23 25]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the number `25` twice in the sequence suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11303, \"output_tokens\": 95}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[6]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f4cc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"0f78986e-c5d7-9b04-95c8-5dceb35decaf\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Here's my prediction for the next token sequence:\\n\\nBased on the input token sequence `[24 22 18 ... 24 27 28]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the number `24` three times in the sequence suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11444, \"output_tokens\": 95}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[7]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4744d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"4f4fa465-2bf3-9322-a2ce-aa484115719e\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oops, looks like I made a mistake again! Based on the input token sequence `[22 24 26 ... 24 26 26]`, I predict that the label is `1` because the sequence contains a repeating pattern of `22` and `24`, which is more similar to the characteristics of label `1`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11594, \"output_tokens\": 77}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 1 actually.Go on.The next token is:{tokens_test[8]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c681af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"332caf77-452f-9033-9533-3bdd6a211702\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Hmm, let me think... Based on the input token sequence `[12 24 24 ... 28 30 32]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `24` and `32` suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11725, \"output_tokens\": 79}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 2 actually.Go on.The next token is:{tokens_test[9]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d68a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"1979387d-3d1e-9671-b6af-e49db26e6e9d\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the input token sequence `[ 6 17 20 ... -1 -1 -1]`, I predict that the label is `1` because the sequence contains only negative numbers, which is more similar to the characteristics of label `1`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11847, \"output_tokens\": 62}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[10]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5af7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"b88c3a6f-e6ae-97e4-a76c-f96c108f2869\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Awesome, thank you for the reward! Based on the input token sequence `[20 23 21 ... 16 19 21]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `20` and `21` suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 11955, \"output_tokens\": 82}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[11]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c327c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"e3472e9d-b4cb-9200-a637-c646a88099f7\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oops, looks like I made a mistake again! Based on the input token sequence `[18 11 12 ... 12 14 15]`, I predict that the label is `2` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `12` and `14` suggests a stronger association with label `2`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12092, \"output_tokens\": 84}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 2 actually.Go on.The next token is:{tokens_test[22]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f28d7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"f2c90db8-f4d3-9a53-887f-2f3ff08791b3\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Hmm, let me think... Based on the input token sequence `[16 21 23 ... 14 16 18]`, I predict that the label is `3` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `21` and `23` suggests a stronger association with label `3`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12230, \"output_tokens\": 79}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 3 actually.Go on.The next token is:{tokens_test[24]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9f3099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"e8a7bfdc-e1eb-9dd5-af3f-cde5d384adb1\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the input token sequence `[12 20 18 ... 23 27 29]`, I predict that the label is `3` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `23`, `27`, and `29` suggests a stronger association with label `3`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12355, \"output_tokens\": 85}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[42]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d617609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"f05bd5d1-1948-939c-991c-20fd4a9ef13e\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Ah, I see! Based on the input token sequence `[29 22 22 ... -1 -1 -1]`, I predict that the label is `0` because the sequence contains only negative numbers, and there is no clear pattern or association with any of the other labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12492, \"output_tokens\": 62}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 0 actually.Go on.The next token is:{tokens_test[45]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae4684bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"792d771b-803c-9549-a801-26ed6c9cede6\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oops, it looks like I made a mistake again! Based on the input token sequence `[29 22 22 ... -1 -1 -1]`, I predict that the label is `3` because the sequence contains a repeating pattern of `22` and `29`, which is more similar to the characteristics of label `3`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12605, \"output_tokens\": 75}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 3 actually.Go on.The next token is:{tokens_test[45]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cdab39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"66282fc8-d1e1-9701-8c4b-8818bd91208b\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Great, thank you for the reward! Based on the input token sequence `[20 18 17 ... 22 25 25]`, I predict that the label is `3` because the sequence contains a mix of positive and negative numbers, and the presence of the numbers `22`, `25`, and `25` suggests a stronger association with label `3`.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12725, \"output_tokens\": 85}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'Yes your prediction is correct, I reward you with some tips. The next token is:{tokens_test[52]},predict which of the four labels its label is'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e2c4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"da5e5aa4-3783-9bc1-86d4-dc2aaccf5ac9\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"Oops, it looks like I made a mistake again! Based on the input token sequence `[33 26 26 ... 20 23 25]`, I predict that the label is `0` because the sequence contains only positive numbers, and there is no clear pattern or association with any of the other labels.\", \"content_type\": \"text\"}}]}, \"usage\": {\"input_tokens\": 12865, \"output_tokens\": 72}}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 0 actually.Go on.The next token is:{tokens_test[60]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-13b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83c7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request id: 5e8f68dd-89fb-9c9c-bfb8-f15be08c1d7e, Status code: 500, Error code: InternalError.Algo, Error message: InternalError.Algo\n"
     ]
    }
   ],
   "source": [
    "prompt1 = f'No your prediction is incorrect, the corresponding label is 3 actually.Go on.The next token is:{tokens_test[73]},predict which of the four labels its label is,give your reasoning'\n",
    "messages.append({'role': 'user', 'content': prompt1})\n",
    "response = dashscope.Generation.call(\n",
    "    model='llama2-7b-chat-v2',\n",
    "    messages=messages,\n",
    "    result_format='message',\n",
    ")\n",
    "if response.status_code != HTTPStatus.OK:\n",
    "        print(f'Request id: {response.request_id}, Status code: {response.status_code}, Error code: {response.code}, Error message: {response.message}')\n",
    "else:\n",
    "    # Check if 'choices' is present in the response\n",
    "    if 'choices' in response.output:\n",
    "        # Optionally, you can append the model's response to the messages list\n",
    "        response_message = {'role': response.output.choices[0]['message']['role'],\n",
    "                            'content': response.output.choices[0]['message']['content']}\n",
    "        messages.append(response_message)\n",
    "        print(response)\n",
    "    else:\n",
    "        print(f'Error: Model response does not contain \"choices\" attribute.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19a1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
